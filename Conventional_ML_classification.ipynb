{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skhosanam/ml-dl-classification/blob/main/Conventional_ML_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW8rIZvkH2AX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDga8DYqGmfG",
        "outputId": "04b4ef62-6c66-4397-a877-02c33128d817"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "xqoXz-mMH2o3",
        "outputId": "628e8182-6987-45ad-9509-b74acbba8114"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9b07336b-f7b6-4a3c-966f-af37091e4f92\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9b07336b-f7b6-4a3c-966f-af37091e4f92\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving fakeNews.csv to fakeNews.csv\n",
            "Saving trueNews.csv to trueNews.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "FLaRsT5uH7OI",
        "outputId": "62136df8-3730-4149-897e-da32b3ad38be"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'fakeNews.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c418dea40f63>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfake_news_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fakeNews.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrue_news_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trueNews.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fakeNews.csv'"
          ]
        }
      ],
      "source": [
        "# Load the datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfKZSytbHeUQ"
      },
      "outputs": [],
      "source": [
        "# Pre-processing functions\n",
        "def regex_substitution(text):\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def remove_urls_usernames_hashtags(text):\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    return re.sub(r'#\\w+', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "def remove_extra_spaces(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    df['Text'] = df['Text'].apply(regex_substitution)\n",
        "    df['Text'] = df['Text'].apply(remove_urls_usernames_hashtags)\n",
        "    df['Text'] = df['Text'].apply(str.lower)\n",
        "    df['Text'] = df['Text'].apply(remove_stopwords)\n",
        "    df['Text'] = df['Text'].apply(remove_extra_spaces)\n",
        "    df = df.drop_duplicates(subset='Text')\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqiDJkJUHfOm"
      },
      "outputs": [],
      "source": [
        "# Apply pre-processing\n",
        "preprocessed_fake_news_df = preprocess_dataframe(fake_news_df)\n",
        "preprocessed_true_news_df = preprocess_dataframe(true_news_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfH--x1IIOf1"
      },
      "outputs": [],
      "source": [
        "# Combine datasets and split into training and testing sets\n",
        "combined_df = pd.concat([preprocessed_fake_news_df.assign(label=0), preprocessed_true_news_df.assign(label=1)])\n",
        "texts = combined_df['Text']\n",
        "labels = combined_df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gidO5WhyIMrH"
      },
      "outputs": [],
      "source": [
        "# Vectorize the text using BoW and TF-IDF\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_test_counts = count_vect.transform(X_test)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNVlA3aXIWR9"
      },
      "outputs": [],
      "source": [
        "# Define and train classifiers\n",
        "classifiers = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'SGD': SGDClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'MLP': MLPClassifier()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCIJq1aUIWAQ"
      },
      "outputs": [],
      "source": [
        "# Evaluate classifiers\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train_tfidf, y_train)\n",
        "    y_pred = clf.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    results[name] = {'Accuracy': accuracy, 'F1 Score': f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOZ62uzTIVr1",
        "outputId": "da0289cb-5672-4c9f-9f96-137ca661e361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Naive Bayes': {'Accuracy': 0.8944793850454228, 'F1 Score': 0.8995342648037259}, 'Gradient Boosting': {'Accuracy': 0.9063591893780573, 'F1 Score': 0.9158291457286433}, 'SVM': {'Accuracy': 0.9308176100628931, 'F1 Score': 0.9358392741412832}, 'Decision Tree': {'Accuracy': 0.8909853249475891, 'F1 Score': 0.8941655359565808}, 'Random Forest': {'Accuracy': 0.9238294898672257, 'F1 Score': 0.927766732935719}, 'Bagging': {'Accuracy': 0.9042627533193571, 'F1 Score': 0.9084836339345358}, 'AdaBoost': {'Accuracy': 0.9133473095737247, 'F1 Score': 0.9191655801825294}, 'SGD': {'Accuracy': 0.9280223619846262, 'F1 Score': 0.9322813938198554}, 'Logistic Regression': {'Accuracy': 0.9224318658280922, 'F1 Score': 0.9279688513951979}, 'MLP': {'Accuracy': 0.9014675052410901, 'F1 Score': 0.9030927835051547}}\n"
          ]
        }
      ],
      "source": [
        "# Display results\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBgx_1qvGWlT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxOUzDiNGWnR",
        "outputId": "6d6c674d-9ba5-4b36-fe0e-e2895b99c347"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Naive Bayes': {'Accuracy': 0.9084556254367575, 'F1 Score': 0.9146579804560261}, 'Gradient Boosting': {'Accuracy': 0.9140461215932913, 'F1 Score': 0.9213051823416506}, 'SVM': {'Accuracy': 0.9182389937106918, 'F1 Score': 0.9255251432208784}, 'Decision Tree': {'Accuracy': 0.8469601677148847, 'F1 Score': 0.8460997891777933}, 'Random Forest': {'Accuracy': 0.9175401816911251, 'F1 Score': 0.9191780821917808}, 'Bagging': {'Accuracy': 0.883298392732355, 'F1 Score': 0.8837856645789841}, 'AdaBoost': {'Accuracy': 0.909853249475891, 'F1 Score': 0.9146260754467239}, 'SGD': {'Accuracy': 0.9315164220824598, 'F1 Score': 0.9367741935483871}, 'Logistic Regression': {'Accuracy': 0.9105520614954578, 'F1 Score': 0.9184713375796177}, 'MLP': {'Accuracy': 0.9196366177498253, 'F1 Score': 0.9225589225589226}}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Pre-processing functions\n",
        "def regex_substitution(text):\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def remove_urls_usernames_hashtags(text):\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    return re.sub(r'#\\w+', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "def remove_extra_spaces(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    df['Text'] = df['Text'].apply(regex_substitution)\n",
        "    df['Text'] = df['Text'].apply(remove_urls_usernames_hashtags)\n",
        "    df['Text'] = df['Text'].apply(str.lower)\n",
        "    df['Text'] = df['Text'].apply(remove_stopwords)\n",
        "    df['Text'] = df['Text'].apply(remove_extra_spaces)\n",
        "    df = df.drop_duplicates(subset='Text')\n",
        "    return df\n",
        "\n",
        "# Apply pre-processing\n",
        "preprocessed_fake_news_df = preprocess_dataframe(fake_news_df)\n",
        "preprocessed_true_news_df = preprocess_dataframe(true_news_df)\n",
        "\n",
        "# Combine datasets and split into training and testing sets\n",
        "combined_df = pd.concat([preprocessed_fake_news_df.assign(label=0), preprocessed_true_news_df.assign(label=1)])\n",
        "texts = combined_df['Text']\n",
        "labels = combined_df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text using n-grams BoW and TF-IDF\n",
        "count_vect = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "X_test_counts = count_vect.transform(X_test)  # Corrected line\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)  # Using the same transformer\n",
        "\n",
        "\n",
        "# Define and train classifiers\n",
        "classifiers = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'SGD': SGDClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'MLP': MLPClassifier()\n",
        "}\n",
        "\n",
        "# Evaluate classifiers\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train_tfidf, y_train)\n",
        "    y_pred = clf.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    results[name] = {'Accuracy': accuracy, 'F1 Score': f1}\n",
        "\n",
        "# Display results\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08LMVGe8GWv-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF6ya9EnRQlI",
        "outputId": "31bd5014-d233-4b16-fcc1-ff6e6867e803"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Naive Bayes': {'Accuracy': 0.9106769016050245, 'F1 Score': 0.9138627187079408}, 'Gradient Boosting': {'Accuracy': 0.9002093510118633, 'F1 Score': 0.90473017988008}, 'SVM': {'Accuracy': 0.9218422889043963, 'F1 Score': 0.9255319148936171}, 'Decision Tree': {'Accuracy': 0.8583391486392185, 'F1 Score': 0.8519328956965718}, 'Random Forest': {'Accuracy': 0.900907187718074, 'F1 Score': 0.8957415565345079}, 'Bagging': {'Accuracy': 0.8729937194696441, 'F1 Score': 0.8667642752562226}, 'AdaBoost': {'Accuracy': 0.8667131891137474, 'F1 Score': 0.8616944243301955}, 'SGD': {'Accuracy': 0.9385903698534543, 'F1 Score': 0.9407806191117093}, 'Logistic Regression': {'Accuracy': 0.9120725750174459, 'F1 Score': 0.9159999999999999}, 'MLP': {'Accuracy': 0.9316120027913468, 'F1 Score': 0.9316596931659694}}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Custom list of stopwords (fewer than the complete list)\n",
        "custom_stopwords = ['the', 'is', 'at', 'of', 'on', 'and', 'a', 'in']\n",
        "\n",
        "# Pre-processing functions\n",
        "def regex_substitution(text):\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def remove_urls_usernames_hashtags(text):\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    return re.sub(r'#\\w+', '', text)\n",
        "\n",
        "def remove_custom_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in custom_stopwords])\n",
        "\n",
        "def remove_extra_spaces(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    df['Text'] = df['Text'].apply(regex_substitution)\n",
        "    df['Text'] = df['Text'].apply(remove_urls_usernames_hashtags)\n",
        "    df['Text'] = df['Text'].apply(str.lower)\n",
        "    df['Text'] = df['Text'].apply(remove_custom_stopwords)\n",
        "    df['Text'] = df['Text'].apply(remove_extra_spaces)\n",
        "    df = df.drop_duplicates(subset='Text')\n",
        "    return df\n",
        "\n",
        "# Load the datasets (replace with the correct file paths)\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Apply pre-processing\n",
        "preprocessed_fake_news_df = preprocess_dataframe(fake_news_df)\n",
        "preprocessed_true_news_df = preprocess_dataframe(true_news_df)\n",
        "\n",
        "# Combine datasets and split into training and testing sets\n",
        "combined_df = pd.concat([preprocessed_fake_news_df.assign(label=0), preprocessed_true_news_df.assign(label=1)])\n",
        "texts = combined_df['Text']\n",
        "labels = combined_df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text using n-grams BoW and TF-IDF\n",
        "count_vect = CountVectorizer(ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_test_counts = count_vect.transform(X_test)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "\n",
        "# Define and train classifiers\n",
        "classifiers = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'SGD': SGDClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'MLP': MLPClassifier()\n",
        "}\n",
        "\n",
        "# Evaluate classifiers\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train_tfidf, y_train)\n",
        "    y_pred = clf.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    results[name] = {'Accuracy': accuracy, 'F1 Score': f1}\n",
        "\n",
        "# Display results\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SkyGoDJRQn6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSvRJW95UR45",
        "outputId": "b1c602fe-c95f-4b28-9607-84fb3073f5e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Naive Bayes': {'Accuracy': 0.902370990237099, 'F1 Score': 0.9060402684563758}, 'Gradient Boosting': {'Accuracy': 0.9142259414225942, 'F1 Score': 0.9202851587815943}, 'SVM': {'Accuracy': 0.9414225941422594, 'F1 Score': 0.9441489361702128}, 'Decision Tree': {'Accuracy': 0.8807531380753139, 'F1 Score': 0.8798313422347154}, 'Random Forest': {'Accuracy': 0.9351464435146444, 'F1 Score': 0.9350104821802936}, 'Bagging': {'Accuracy': 0.9058577405857741, 'F1 Score': 0.906184850590688}, 'AdaBoost': {'Accuracy': 0.9191073919107392, 'F1 Score': 0.921195652173913}, 'SGD': {'Accuracy': 0.9372384937238494, 'F1 Score': 0.9400798934753662}, 'Logistic Regression': {'Accuracy': 0.9309623430962343, 'F1 Score': 0.934131736526946}, 'MLP': {'Accuracy': 0.9302649930264993, 'F1 Score': 0.9304589707927677}}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Pre-processing functions\n",
        "def regex_substitution(text):\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def remove_urls_usernames_hashtags(text):\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    return re.sub(r'#\\w+', '', text)\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "def remove_extra_spaces(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    df['Text'] = df['Text'].apply(regex_substitution)\n",
        "    df['Text'] = df['Text'].apply(remove_urls_usernames_hashtags)\n",
        "    df['Text'] = df['Text'].apply(str.lower)\n",
        "    df['Text'] = df['Text'].apply(lemmatize_text)\n",
        "    df['Text'] = df['Text'].apply(remove_extra_spaces)\n",
        "    df = df.drop_duplicates(subset='Text')\n",
        "    return df\n",
        "\n",
        "# Load the datasets (replace with the correct file paths)\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Apply pre-processing\n",
        "preprocessed_fake_news_df = preprocess_dataframe(fake_news_df)\n",
        "preprocessed_true_news_df = preprocess_dataframe(true_news_df)\n",
        "\n",
        "# Combine datasets and split into training and testing sets\n",
        "combined_df = pd.concat([preprocessed_fake_news_df.assign(label=0), preprocessed_true_news_df.assign(label=1)])\n",
        "texts = combined_df['Text']\n",
        "labels = combined_df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text using BoW and TF-IDF\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_test_counts = count_vect.transform(X_test)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "\n",
        "# Define and train classifiers\n",
        "classifiers = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'SGD': SGDClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'MLP': MLPClassifier()\n",
        "}\n",
        "\n",
        "# Evaluate classifiers\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train_tfidf, y_train)\n",
        "    y_pred = clf.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    results[name] = {'Accuracy': accuracy, 'F1 Score': f1}\n",
        "\n",
        "# Display results\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7aoSmTTWB9R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GojzbFGlWCNJ",
        "outputId": "be189a35-08b4-4ea4-aae7-6a017c73207f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Naive Bayes': {'Accuracy': 0.9177126917712691, 'F1 Score': 0.9193989071038252}, 'Gradient Boosting': {'Accuracy': 0.9135285913528591, 'F1 Score': 0.9187418086500655}, 'SVM': {'Accuracy': 0.9386331938633193, 'F1 Score': 0.9410187667560321}, 'Decision Tree': {'Accuracy': 0.8807531380753139, 'F1 Score': 0.8808362369337979}, 'Random Forest': {'Accuracy': 0.9358437935843794, 'F1 Score': 0.9361997226074896}, 'Bagging': {'Accuracy': 0.9114365411436541, 'F1 Score': 0.9131920710868079}, 'AdaBoost': {'Accuracy': 0.9149232914923291, 'F1 Score': 0.9168937329700273}, 'SGD': {'Accuracy': 0.9330543933054394, 'F1 Score': 0.9350473612990529}, 'Logistic Regression': {'Accuracy': 0.9267782426778243, 'F1 Score': 0.9296718017414601}, 'MLP': {'Accuracy': 0.9128312412831241, 'F1 Score': 0.9128919860627178}}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Pre-processing functions\n",
        "def regex_substitution(text):\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "def remove_urls_usernames_hashtags(text):\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    return re.sub(r'#\\w+', '', text)\n",
        "\n",
        "def stem_text(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([stemmer.stem(word) for word in words])\n",
        "\n",
        "def remove_extra_spaces(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    df['Text'] = df['Text'].apply(regex_substitution)\n",
        "    df['Text'] = df['Text'].apply(remove_urls_usernames_hashtags)\n",
        "    df['Text'] = df['Text'].apply(str.lower)\n",
        "    df['Text'] = df['Text'].apply(stem_text)\n",
        "    df['Text'] = df['Text'].apply(remove_extra_spaces)\n",
        "    df = df.drop_duplicates(subset='Text')\n",
        "    return df\n",
        "\n",
        "# Load the datasets (replace with the correct file paths)\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Apply pre-processing\n",
        "preprocessed_fake_news_df = preprocess_dataframe(fake_news_df)\n",
        "preprocessed_true_news_df = preprocess_dataframe(true_news_df)\n",
        "\n",
        "# Combine datasets and split into training and testing sets\n",
        "combined_df = pd.concat([preprocessed_fake_news_df.assign(label=0), preprocessed_true_news_df.assign(label=1)])\n",
        "texts = combined_df['Text']\n",
        "labels = combined_df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the text using BoW and TF-IDF\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(X_train)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_test_counts = count_vect.transform(X_test)\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "\n",
        "# Define and train classifiers\n",
        "classifiers = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Bagging': BaggingClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'SGD': SGDClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'MLP': MLPClassifier()\n",
        "}\n",
        "\n",
        "# Evaluate classifiers\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(X_train_tfidf, y_train)\n",
        "    y_pred = clf.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    results[name] = {'Accuracy': accuracy, 'F1 Score': f1}\n",
        "\n",
        "# Display results\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReQcA0oeLIUF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nrMYofGLIWn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "g5XqAuXQLKE8",
        "outputId": "0dbbee82-8ac0-4528-a331-a95ebf00d586"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1725e7c0-c380-48ee-8d98-05d3a0cac918\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1725e7c0-c380-48ee-8d98-05d3a0cac918\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving fakeNews.csv to fakeNews (1).csv\n",
            "Saving trueNews.csv to trueNews (1).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF2WSjcTLeIF"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGHVvX2QMf43",
        "outputId": "35342e7e-8473-42c7-bd90-d734278ecbef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of NaN values in 'Text': 0\n",
            "Number of NaN values in 'Region': 0\n",
            "Size of DataFrame after preprocessing: (7588, 15)\n",
            "\n",
            "Accuracy Results for each Classifier and N-gram Range:\n",
            "{'Multinomial NB': [0.9169960474308301, 0.932806324110672, 0.9308300395256917, 0.9183135704874835], 'Logistic Regression': [0.9472990777338604, 0.9499341238471674, 0.9479578392621871, 0.9446640316205533], 'Passive Aggressive Classifier': [0.9407114624505929, 0.950592885375494, 0.953227931488801, 0.9545454545454546], 'Decision Tree Classifier': [0.9110671936758893, 0.9525691699604744, 0.9479578392621871, 0.9538866930171278], 'Gradient Boosting Classifier': [0.9018445322793149, 0.9202898550724637, 0.9216073781291173, 0.9202898550724637], 'Random Forest Classifier': [0.9413702239789197, 0.9525691699604744, 0.9459815546772069, 0.9479578392621871], 'K-Nearest Neighbor': [0.6640316205533597, 0.5981554677206851, 0.5507246376811594, 0.5329380764163373], 'Support Vector Machine': [0.9407114624505929, 0.9433465085638999, 0.9380764163372859, 0.930171277997365]}\n",
            "\n",
            "Average Accuracy for each Classifier:\n",
            "{'Multinomial NB': 0.9247364953886692, 'Logistic Regression': 0.947463768115942, 'Passive Aggressive Classifier': 0.9497694334650856, 'Decision Tree Classifier': 0.9413702239789197, 'Gradient Boosting Classifier': 0.91600790513834, 'Random Forest Classifier': 0.946969696969697, 'K-Nearest Neighbor': 0.5864624505928854, 'Support Vector Machine': 0.9380764163372859}\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Basic cleaning: remove special characters and convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    return text\n",
        "\n",
        "# Load datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Add label column\n",
        "fake_news_df['Label'] = 0\n",
        "true_news_df['Label'] = 1\n",
        "\n",
        "# Concatenate datasets\n",
        "combined_df = pd.concat([fake_news_df, true_news_df], ignore_index=True)\n",
        "\n",
        "# Check for NaN values in 'Text' and 'Region' columns\n",
        "print(\"Number of NaN values in 'Text':\", combined_df['Text'].isna().sum())\n",
        "print(\"Number of NaN values in 'Region':\", combined_df['Region'].isna().sum())\n",
        "\n",
        "# Drop rows with NaN values in 'Text' or 'Region'\n",
        "combined_df.dropna(subset=['Text', 'Region'], inplace=True)\n",
        "\n",
        "# Apply preprocessing\n",
        "combined_df['Processed_Content'] = combined_df['Text'] + \" \" + combined_df['Region']\n",
        "combined_df['Processed_Content'] = combined_df['Processed_Content'].apply(preprocess_text)\n",
        "\n",
        "# Check the size of the DataFrame after preprocessing\n",
        "print(\"Size of DataFrame after preprocessing:\", combined_df.shape)\n",
        "\n",
        "# Define n-gram ranges and classifiers\n",
        "ngram_ranges = [(1, 1), (1, 2), (1, 3), (1, 4)]\n",
        "classifiers = {\n",
        "    \"Multinomial NB\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Passive Aggressive Classifier\": PassiveAggressiveClassifier(max_iter=1000),\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "    \"K-Nearest Neighbor\": KNeighborsClassifier(),\n",
        "    \"Support Vector Machine\": SVC()\n",
        "}\n",
        "# Function to create n-gram features\n",
        "def create_ngram_features(data, ngram_range):\n",
        "    vectorizer = CountVectorizer(ngram_range=ngram_range, min_df=1)\n",
        "    return vectorizer.fit_transform(data)\n",
        "\n",
        "# Function to evaluate a classifier\n",
        "def evaluate_classifier(X, y, classifier):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    classifier.fit(X_train, y_train)\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Prepare data\n",
        "processed_content = combined_df['Processed_Content']\n",
        "y = combined_df['Label']\n",
        "\n",
        "# Evaluate classifiers on different n-gram feature matrices\n",
        "accuracy_results = {name: [] for name in classifiers.keys()}\n",
        "for ngram_range in ngram_ranges:\n",
        "    X_ngram = create_ngram_features(processed_content, ngram_range)\n",
        "    for name, classifier in classifiers.items():\n",
        "        accuracy = evaluate_classifier(X_ngram, y, classifier)\n",
        "        accuracy_results[name].append(accuracy)\n",
        "\n",
        "# Calculate the average accuracy for each classifier\n",
        "average_accuracies = {name: sum(acc_list) / len(acc_list) for name, acc_list in accuracy_results.items()}\n",
        "\n",
        "print(\"\\nAccuracy Results for each Classifier and N-gram Range:\")\n",
        "print(accuracy_results)\n",
        "print(\"\\nAverage Accuracy for each Classifier:\")\n",
        "print(average_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG43iJDWUUEm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDbButjRUURe",
        "outputId": "0537ca32-9c51-4c43-c697-c15d3eda5395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating with max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.8964\n",
            "Logistic Regression: Accuracy = 0.9337\n",
            "Passive Aggressive Classifier: Accuracy = 0.9147\n",
            "Decision Tree Classifier: Accuracy = 0.9022\n",
            "Gradient Boosting Classifier: Accuracy = 0.9141\n",
            "Random Forest Classifier: Accuracy = 0.9412\n",
            "K-Nearest Neighbor: Accuracy = 0.8556\n",
            "Support Vector Machine: Accuracy = 0.9358\n",
            "\n",
            "Evaluating with max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9137\n",
            "Logistic Regression: Accuracy = 0.9408\n",
            "Passive Aggressive Classifier: Accuracy = 0.9345\n",
            "Decision Tree Classifier: Accuracy = 0.9076\n",
            "Gradient Boosting Classifier: Accuracy = 0.9112\n",
            "Random Forest Classifier: Accuracy = 0.9431\n",
            "K-Nearest Neighbor: Accuracy = 0.8529\n",
            "Support Vector Machine: Accuracy = 0.9356\n",
            "\n",
            "Evaluating with max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9180\n",
            "Logistic Regression: Accuracy = 0.9403\n",
            "Passive Aggressive Classifier: Accuracy = 0.9391\n",
            "Decision Tree Classifier: Accuracy = 0.9120\n",
            "Gradient Boosting Classifier: Accuracy = 0.9097\n",
            "Random Forest Classifier: Accuracy = 0.9436\n",
            "K-Nearest Neighbor: Accuracy = 0.8457\n",
            "Support Vector Machine: Accuracy = 0.9362\n",
            "\n",
            "Evaluating with max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9217\n",
            "Logistic Regression: Accuracy = 0.9359\n",
            "Passive Aggressive Classifier: Accuracy = 0.9394\n",
            "Decision Tree Classifier: Accuracy = 0.9170\n",
            "Gradient Boosting Classifier: Accuracy = 0.9081\n",
            "Random Forest Classifier: Accuracy = 0.9428\n",
            "K-Nearest Neighbor: Accuracy = 0.7766\n",
            "Support Vector Machine: Accuracy = 0.9279\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    return text\n",
        "\n",
        "# Load datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Preprocess data\n",
        "fake_news_df['Label'] = 0\n",
        "true_news_df['Label'] = 1\n",
        "combined_df = pd.concat([fake_news_df, true_news_df], ignore_index=True)\n",
        "combined_df.dropna(subset=['Text', 'Region'], inplace=True)\n",
        "combined_df['Processed_Content'] = combined_df['Text'] + \" \" + combined_df['Region']\n",
        "combined_df['Processed_Content'] = combined_df['Processed_Content'].apply(preprocess_text)\n",
        "\n",
        "# Prepare data\n",
        "processed_content = combined_df['Processed_Content']\n",
        "y = combined_df['Label']\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"Multinomial NB\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Passive Aggressive Classifier\": PassiveAggressiveClassifier(max_iter=1000),\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "    \"K-Nearest Neighbor\": KNeighborsClassifier(),\n",
        "    \"Support Vector Machine\": SVC()\n",
        "}\n",
        "\n",
        "# Function to perform cross-validation\n",
        "def perform_cross_validation(classifier, X, y, cv=5):\n",
        "    scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "# N-gram range and maximum features settings\n",
        "ngram_range = (1, 4)\n",
        "max_features_options = [1000, 5000, 10000, 50000]\n",
        "\n",
        "# Evaluate classifiers with different max_features settings\n",
        "for max_features in max_features_options:\n",
        "    print(f\"\\nEvaluating with max_features={max_features}:\")\n",
        "    vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "    X = vectorizer.fit_transform(processed_content)\n",
        "\n",
        "    for name, classifier in classifiers.items():\n",
        "        accuracy = perform_cross_validation(classifier, X, y, cv=5)\n",
        "        print(f\"{name}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0AMpxZDWMw3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYBJIHrmWM-g",
        "outputId": "4f7cbbfe-4714-40e2-9b31-5ab542f496bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating with max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.8923\n",
            "Logistic Regression: Accuracy = 0.9288\n",
            "Passive Aggressive Classifier: Accuracy = 0.8993\n",
            "Decision Tree Classifier: Accuracy = 0.8905\n",
            "Gradient Boosting Classifier: Accuracy = 0.9095\n",
            "Random Forest Classifier: Accuracy = 0.9294\n",
            "K-Nearest Neighbor: Accuracy = 0.8780\n",
            "Support Vector Machine: Accuracy = 0.9274\n",
            "\n",
            "Evaluating with max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9157\n",
            "Logistic Regression: Accuracy = 0.9359\n",
            "Passive Aggressive Classifier: Accuracy = 0.9247\n",
            "Decision Tree Classifier: Accuracy = 0.9031\n",
            "Gradient Boosting Classifier: Accuracy = 0.9070\n",
            "Random Forest Classifier: Accuracy = 0.9371\n",
            "K-Nearest Neighbor: Accuracy = 0.8536\n",
            "Support Vector Machine: Accuracy = 0.9257\n",
            "\n",
            "Evaluating with max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9193\n",
            "Logistic Regression: Accuracy = 0.9363\n",
            "Passive Aggressive Classifier: Accuracy = 0.9315\n",
            "Decision Tree Classifier: Accuracy = 0.9088\n",
            "Gradient Boosting Classifier: Accuracy = 0.9044\n",
            "Random Forest Classifier: Accuracy = 0.9363\n",
            "K-Nearest Neighbor: Accuracy = 0.8437\n",
            "Support Vector Machine: Accuracy = 0.9237\n",
            "\n",
            "Evaluating with max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9226\n",
            "Logistic Regression: Accuracy = 0.9345\n",
            "Passive Aggressive Classifier: Accuracy = 0.9352\n",
            "Decision Tree Classifier: Accuracy = 0.9117\n",
            "Gradient Boosting Classifier: Accuracy = 0.9046\n",
            "Random Forest Classifier: Accuracy = 0.9392\n",
            "K-Nearest Neighbor: Accuracy = 0.6888\n",
            "Support Vector Machine: Accuracy = 0.9149\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stop words and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Special Characters Removal and Lower Casing\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Stop Word Removal and Stemming\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Rejoin tokens into a string\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "# Load datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Preprocess data\n",
        "fake_news_df['Label'] = 0\n",
        "true_news_df['Label'] = 1\n",
        "combined_df = pd.concat([fake_news_df, true_news_df], ignore_index=True)\n",
        "combined_df.dropna(subset=['Text', 'Region'], inplace=True)\n",
        "combined_df['Processed_Content'] = combined_df['Text'] + \" \" + combined_df['Region']\n",
        "combined_df['Processed_Content'] = combined_df['Processed_Content'].apply(preprocess_text)\n",
        "\n",
        "# Prepare data\n",
        "processed_content = combined_df['Processed_Content']\n",
        "y = combined_df['Label']\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"Multinomial NB\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Passive Aggressive Classifier\": PassiveAggressiveClassifier(max_iter=1000),\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "    \"K-Nearest Neighbor\": KNeighborsClassifier(),\n",
        "    \"Support Vector Machine\": SVC()\n",
        "}\n",
        "\n",
        "# Function to perform cross-validation\n",
        "def perform_cross_validation(classifier, X, y, cv=5):\n",
        "    scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "# N-gram range and maximum features settings\n",
        "ngram_range = (1, 4)\n",
        "max_features_options = [1000, 5000, 10000, 50000]\n",
        "\n",
        "# Evaluate classifiers with different max_features settings\n",
        "for max_features in max_features_options:\n",
        "    print(f\"\\nEvaluating with max_features={max_features}:\")\n",
        "    vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "    X = vectorizer.fit_transform(processed_content)\n",
        "\n",
        "    for name, classifier in classifiers.items():\n",
        "        accuracy = perform_cross_validation(classifier, X, y, cv=5)\n",
        "        print(f\"{name}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etS3Sor5Ydlw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnEbTZOEYeKH",
        "outputId": "e5f13569-690b-4bad-86f7-adcb1d1cca68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating with max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.8998\n",
            "Logistic Regression: Accuracy = 0.9411\n",
            "Passive Aggressive Classifier: Accuracy = 0.9141\n",
            "Decision Tree Classifier: Accuracy = 0.9095\n",
            "Gradient Boosting Classifier: Accuracy = 0.9187\n",
            "Random Forest Classifier: Accuracy = 0.9454\n",
            "K-Nearest Neighbor: Accuracy = 0.8893\n",
            "Support Vector Machine: Accuracy = 0.9407\n",
            "\n",
            "Evaluating with max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9305\n",
            "Logistic Regression: Accuracy = 0.9528\n",
            "Passive Aggressive Classifier: Accuracy = 0.9437\n",
            "Decision Tree Classifier: Accuracy = 0.9271\n",
            "Gradient Boosting Classifier: Accuracy = 0.9159\n",
            "Random Forest Classifier: Accuracy = 0.9528\n",
            "K-Nearest Neighbor: Accuracy = 0.8719\n",
            "Support Vector Machine: Accuracy = 0.9456\n",
            "\n",
            "Evaluating with max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9360\n",
            "Logistic Regression: Accuracy = 0.9537\n",
            "Passive Aggressive Classifier: Accuracy = 0.9486\n",
            "Decision Tree Classifier: Accuracy = 0.9279\n",
            "Gradient Boosting Classifier: Accuracy = 0.9161\n",
            "Random Forest Classifier: Accuracy = 0.9520\n",
            "K-Nearest Neighbor: Accuracy = 0.8662\n",
            "Support Vector Machine: Accuracy = 0.9461\n",
            "\n",
            "Evaluating with max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9427\n",
            "Logistic Regression: Accuracy = 0.9528\n",
            "Passive Aggressive Classifier: Accuracy = 0.9507\n",
            "Decision Tree Classifier: Accuracy = 0.9315\n",
            "Gradient Boosting Classifier: Accuracy = 0.9163\n",
            "Random Forest Classifier: Accuracy = 0.9532\n",
            "K-Nearest Neighbor: Accuracy = 0.7203\n",
            "Support Vector Machine: Accuracy = 0.9418\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stop words and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    tokens = text.split()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "# Load datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Add label column and drop unnecessary features\n",
        "fake_news_df = fake_news_df[['Text', 'Region']].dropna()\n",
        "fake_news_df['Label'] = 0\n",
        "\n",
        "true_news_df = true_news_df[['Text', 'Region']].dropna()\n",
        "true_news_df['Label'] = 1\n",
        "\n",
        "# Concatenate datasets\n",
        "combined_df = pd.concat([fake_news_df, true_news_df], ignore_index=True)\n",
        "\n",
        "# Create Processed_Content feature\n",
        "combined_df['Processed_Content'] = combined_df['Text'] + \" \" + combined_df['Region']\n",
        "combined_df['Processed_Content'] = combined_df['Processed_Content'].apply(preprocess_text)\n",
        "\n",
        "# Shuffle the data\n",
        "combined_df = shuffle(combined_df, random_state=42)\n",
        "\n",
        "# Prepare data\n",
        "processed_content = combined_df['Processed_Content']\n",
        "y = combined_df['Label']\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"Multinomial NB\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Passive Aggressive Classifier\": PassiveAggressiveClassifier(max_iter=1000),\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "    \"K-Nearest Neighbor\": KNeighborsClassifier(),\n",
        "    \"Support Vector Machine\": SVC()\n",
        "}\n",
        "\n",
        "# Function to perform cross-validation\n",
        "def perform_cross_validation(classifier, X, y, cv=5):\n",
        "    scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "# N-gram range and maximum features settings\n",
        "ngram_range = (1, 4)\n",
        "max_features_options = [1000, 5000, 10000, 50000]\n",
        "\n",
        "# Evaluate classifiers with different max_features settings\n",
        "for max_features in max_features_options:\n",
        "    print(f\"\\nEvaluating with max_features={max_features}:\")\n",
        "    vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "    X = vectorizer.fit_transform(processed_content)\n",
        "\n",
        "    for name, classifier in classifiers.items():\n",
        "        accuracy = perform_cross_validation(classifier, X, y, cv=5)\n",
        "        print(f\"{name}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PdB3MtcZ2wQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU03uSIjZ26K",
        "outputId": "543ee1be-5bac-454c-ba45-935f638a8eac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating with max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.9034\n",
            "Logistic Regression: Accuracy = 0.9415\n",
            "Passive Aggressive Classifier: Accuracy = 0.9146\n",
            "Decision Tree Classifier: Accuracy = 0.9109\n",
            "Gradient Boosting Classifier: Accuracy = 0.9197\n",
            "Random Forest Classifier: Accuracy = 0.9449\n",
            "K-Nearest Neighbor: Accuracy = 0.8909\n",
            "Support Vector Machine: Accuracy = 0.9419\n",
            "\n",
            "Evaluating with max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9303\n",
            "Logistic Regression: Accuracy = 0.9515\n",
            "Passive Aggressive Classifier: Accuracy = 0.9448\n",
            "Decision Tree Classifier: Accuracy = 0.9279\n",
            "Gradient Boosting Classifier: Accuracy = 0.9179\n",
            "Random Forest Classifier: Accuracy = 0.9512\n",
            "K-Nearest Neighbor: Accuracy = 0.8807\n",
            "Support Vector Machine: Accuracy = 0.9454\n",
            "\n",
            "Evaluating with max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9381\n",
            "Logistic Regression: Accuracy = 0.9543\n",
            "Passive Aggressive Classifier: Accuracy = 0.9490\n",
            "Decision Tree Classifier: Accuracy = 0.9312\n",
            "Gradient Boosting Classifier: Accuracy = 0.9146\n",
            "Random Forest Classifier: Accuracy = 0.9519\n",
            "K-Nearest Neighbor: Accuracy = 0.8784\n",
            "Support Vector Machine: Accuracy = 0.9461\n",
            "\n",
            "Evaluating with max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9441\n",
            "Logistic Regression: Accuracy = 0.9522\n",
            "Passive Aggressive Classifier: Accuracy = 0.9504\n",
            "Decision Tree Classifier: Accuracy = 0.9311\n",
            "Gradient Boosting Classifier: Accuracy = 0.9162\n",
            "Random Forest Classifier: Accuracy = 0.9523\n",
            "K-Nearest Neighbor: Accuracy = 0.7163\n",
            "Support Vector Machine: Accuracy = 0.9410\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Initialize stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Load datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Preprocess data\n",
        "fake_news_df['Label'] = 0\n",
        "true_news_df['Label'] = 1\n",
        "combined_df = pd.concat([fake_news_df, true_news_df], ignore_index=True)\n",
        "combined_df.dropna(subset=['Text', 'Region'], inplace=True)\n",
        "combined_df['Processed_Content'] = combined_df['Text'] + \" \" + combined_df['Region']\n",
        "combined_df['Processed_Content'] = combined_df['Processed_Content'].apply(preprocess_text)\n",
        "\n",
        "# Shuffle the data\n",
        "combined_df = shuffle(combined_df, random_state=42)\n",
        "\n",
        "# Prepare data\n",
        "processed_content = combined_df['Processed_Content']\n",
        "y = combined_df['Label']\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"Multinomial NB\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Passive Aggressive Classifier\": PassiveAggressiveClassifier(max_iter=1000),\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "    \"K-Nearest Neighbor\": KNeighborsClassifier(),\n",
        "    \"Support Vector Machine\": SVC()\n",
        "}\n",
        "\n",
        "# Function to perform cross-validation\n",
        "def perform_cross_validation(classifier, X, y, cv=5):\n",
        "    scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "# N-gram range and maximum features settings\n",
        "ngram_range = (1, 4)\n",
        "max_features_options = [1000, 5000, 10000, 50000]\n",
        "\n",
        "# Evaluate classifiers with different max_features settings\n",
        "for max_features in max_features_options:\n",
        "    print(f\"\\nEvaluating with max_features={max_features}:\")\n",
        "    vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "    X = vectorizer.fit_transform(processed_content)\n",
        "\n",
        "    for name, classifier in classifiers.items():\n",
        "        accuracy = perform_cross_validation(classifier, X, y, cv=5)\n",
        "        print(f\"{name}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTUu_65okRkD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peULUSD6kR6l",
        "outputId": "e7ac1923-249f-44c5-a779-62983ab7ecb5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating with max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.8969\n",
            "Logistic Regression: Accuracy = 0.9373\n",
            "Passive Aggressive Classifier: Accuracy = 0.9167\n",
            "Decision Tree Classifier: Accuracy = 0.9149\n",
            "Gradient Boosting Classifier: Accuracy = 0.9162\n",
            "Random Forest Classifier: Accuracy = 0.9448\n",
            "K-Nearest Neighbor: Accuracy = 0.8695\n",
            "Support Vector Machine: Accuracy = 0.9485\n",
            "\n",
            "Evaluating with max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9284\n",
            "Logistic Regression: Accuracy = 0.9482\n",
            "Passive Aggressive Classifier: Accuracy = 0.9468\n",
            "Decision Tree Classifier: Accuracy = 0.9155\n",
            "Gradient Boosting Classifier: Accuracy = 0.9167\n",
            "Random Forest Classifier: Accuracy = 0.9501\n",
            "K-Nearest Neighbor: Accuracy = 0.8898\n",
            "Support Vector Machine: Accuracy = 0.9593\n",
            "\n",
            "Evaluating with max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9363\n",
            "Logistic Regression: Accuracy = 0.9501\n",
            "Passive Aggressive Classifier: Accuracy = 0.9551\n",
            "Decision Tree Classifier: Accuracy = 0.9242\n",
            "Gradient Boosting Classifier: Accuracy = 0.9159\n",
            "Random Forest Classifier: Accuracy = 0.9501\n",
            "K-Nearest Neighbor: Accuracy = 0.9000\n",
            "Support Vector Machine: Accuracy = 0.9586\n",
            "\n",
            "Evaluating with max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9399\n",
            "Logistic Regression: Accuracy = 0.9493\n",
            "Passive Aggressive Classifier: Accuracy = 0.9593\n",
            "Decision Tree Classifier: Accuracy = 0.9203\n",
            "Gradient Boosting Classifier: Accuracy = 0.9159\n",
            "Random Forest Classifier: Accuracy = 0.9485\n",
            "K-Nearest Neighbor: Accuracy = 0.9017\n",
            "Support Vector Machine: Accuracy = 0.9559\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Initialize stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Load datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Preprocess data\n",
        "fake_news_df['Label'] = 0\n",
        "true_news_df['Label'] = 1\n",
        "combined_df = pd.concat([fake_news_df, true_news_df], ignore_index=True)\n",
        "combined_df.dropna(subset=['Text', 'Region'], inplace=True)\n",
        "combined_df['Processed_Content'] = combined_df['Text'] + \" \" + combined_df['Region']\n",
        "combined_df['Processed_Content'] = combined_df['Processed_Content'].apply(preprocess_text)\n",
        "\n",
        "# Shuffle the data\n",
        "combined_df = shuffle(combined_df, random_state=42)\n",
        "\n",
        "# Prepare data\n",
        "processed_content = combined_df['Processed_Content']\n",
        "y = combined_df['Label']\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"Multinomial NB\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Passive Aggressive Classifier\": PassiveAggressiveClassifier(max_iter=1000),\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "    \"K-Nearest Neighbor\": KNeighborsClassifier(),\n",
        "    \"Support Vector Machine\": SVC()\n",
        "}\n",
        "\n",
        "# Function to perform cross-validation\n",
        "def perform_cross_validation(classifier, X, y, cv=5):\n",
        "    scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "# N-gram range and maximum features settings\n",
        "ngram_range = (1, 4)\n",
        "max_features_options = [1000, 5000, 10000, 50000]\n",
        "\n",
        "# Evaluate classifiers with different max_features settings\n",
        "for max_features in max_features_options:\n",
        "    print(f\"\\nEvaluating with max_features={max_features}:\")\n",
        "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "    X = vectorizer.fit_transform(processed_content)\n",
        "\n",
        "    for name, classifier in classifiers.items():\n",
        "        accuracy = perform_cross_validation(classifier, X, y, cv=5)\n",
        "        print(f\"{name}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOC2RMySlXj9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcmnUKVJlX7D",
        "outputId": "f6625ec9-3613-49ca-e7e9-ea2138125259"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating with max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.8947\n",
            "Logistic Regression: Accuracy = 0.9365\n",
            "Passive Aggressive Classifier: Accuracy = 0.9141\n",
            "Decision Tree Classifier: Accuracy = 0.9168\n",
            "Gradient Boosting Classifier: Accuracy = 0.9170\n",
            "Random Forest Classifier: Accuracy = 0.9431\n",
            "K-Nearest Neighbor: Accuracy = 0.8691\n",
            "Support Vector Machine: Accuracy = 0.9487\n",
            "\n",
            "Evaluating with max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9275\n",
            "Logistic Regression: Accuracy = 0.9486\n",
            "Passive Aggressive Classifier: Accuracy = 0.9462\n",
            "Decision Tree Classifier: Accuracy = 0.9178\n",
            "Gradient Boosting Classifier: Accuracy = 0.9170\n",
            "Random Forest Classifier: Accuracy = 0.9522\n",
            "K-Nearest Neighbor: Accuracy = 0.8927\n",
            "Support Vector Machine: Accuracy = 0.9598\n",
            "\n",
            "Evaluating with max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9344\n",
            "Logistic Regression: Accuracy = 0.9507\n",
            "Passive Aggressive Classifier: Accuracy = 0.9549\n",
            "Decision Tree Classifier: Accuracy = 0.9224\n",
            "Gradient Boosting Classifier: Accuracy = 0.9171\n",
            "Random Forest Classifier: Accuracy = 0.9510\n",
            "K-Nearest Neighbor: Accuracy = 0.9002\n",
            "Support Vector Machine: Accuracy = 0.9590\n",
            "\n",
            "Evaluating with max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9381\n",
            "Logistic Regression: Accuracy = 0.9490\n",
            "Passive Aggressive Classifier: Accuracy = 0.9602\n",
            "Decision Tree Classifier: Accuracy = 0.9241\n",
            "Gradient Boosting Classifier: Accuracy = 0.9164\n",
            "Random Forest Classifier: Accuracy = 0.9489\n",
            "K-Nearest Neighbor: Accuracy = 0.9004\n",
            "Support Vector Machine: Accuracy = 0.9564\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize stop words and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "# Load datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Preprocess data\n",
        "fake_news_df['Label'] = 0\n",
        "true_news_df['Label'] = 1\n",
        "combined_df = pd.concat([fake_news_df, true_news_df], ignore_index=True)\n",
        "combined_df.dropna(subset=['Text', 'Region'], inplace=True)\n",
        "combined_df['Processed_Content'] = combined_df['Text'] + \" \" + combined_df['Region']\n",
        "combined_df['Processed_Content'] = combined_df['Processed_Content'].apply(preprocess_text)\n",
        "\n",
        "# Shuffle the data\n",
        "combined_df = shuffle(combined_df, random_state=42)\n",
        "\n",
        "# Prepare data\n",
        "processed_content = combined_df['Processed_Content']\n",
        "y = combined_df['Label']\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"Multinomial NB\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Passive Aggressive Classifier\": PassiveAggressiveClassifier(max_iter=1000),\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "    \"K-Nearest Neighbor\": KNeighborsClassifier(),\n",
        "    \"Support Vector Machine\": SVC()\n",
        "}\n",
        "\n",
        "# Function to perform cross-validation\n",
        "def perform_cross_validation(classifier, X, y, cv=5):\n",
        "    scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "# N-gram range and maximum features settings\n",
        "ngram_range = (1, 4)\n",
        "max_features_options = [1000, 5000, 10000, 50000]\n",
        "\n",
        "# Evaluate classifiers with different max_features settings\n",
        "for max_features in max_features_options:\n",
        "    print(f\"\\nEvaluating with max_features={max_features}:\")\n",
        "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "    X = vectorizer.fit_transform(processed_content)\n",
        "\n",
        "    for name, classifier in classifiers.items():\n",
        "        accuracy = perform_cross_validation(classifier, X, y, cv=5)\n",
        "        print(f\"{name}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro_p9XWHoSCV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv1bbHQ-oSFW",
        "outputId": "7c1e1869-654b-4f98-a019-5bc90112b206"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating ngram_range=(1, 1), max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.8685\n",
            "Logistic Regression: Accuracy = 0.9225\n",
            "Passive Aggressive Classifier: Accuracy = 0.8962\n",
            "Decision Tree Classifier: Accuracy = 0.9017\n",
            "Gradient Boosting Classifier: Accuracy = 0.9014\n",
            "Random Forest Classifier: Accuracy = 0.9352\n",
            "K-Nearest Neighbor: Accuracy = 0.8388\n",
            "Support Vector Machine: Accuracy = 0.9373\n",
            "\n",
            "Evaluating ngram_range=(1, 1), max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9022\n",
            "Logistic Regression: Accuracy = 0.9294\n",
            "Passive Aggressive Classifier: Accuracy = 0.9269\n",
            "Decision Tree Classifier: Accuracy = 0.9096\n",
            "Gradient Boosting Classifier: Accuracy = 0.9014\n",
            "Random Forest Classifier: Accuracy = 0.9415\n",
            "K-Nearest Neighbor: Accuracy = 0.8616\n",
            "Support Vector Machine: Accuracy = 0.9458\n",
            "\n",
            "Evaluating ngram_range=(1, 1), max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.8950\n",
            "Logistic Regression: Accuracy = 0.9282\n",
            "Passive Aggressive Classifier: Accuracy = 0.9284\n",
            "Decision Tree Classifier: Accuracy = 0.9124\n",
            "Gradient Boosting Classifier: Accuracy = 0.9068\n",
            "Random Forest Classifier: Accuracy = 0.9421\n",
            "K-Nearest Neighbor: Accuracy = 0.8376\n",
            "Support Vector Machine: Accuracy = 0.9424\n",
            "\n",
            "Evaluating ngram_range=(1, 1), max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.8824\n",
            "Logistic Regression: Accuracy = 0.9282\n",
            "Passive Aggressive Classifier: Accuracy = 0.9357\n",
            "Decision Tree Classifier: Accuracy = 0.9205\n",
            "Gradient Boosting Classifier: Accuracy = 0.9191\n",
            "Random Forest Classifier: Accuracy = 0.9508\n",
            "K-Nearest Neighbor: Accuracy = 0.7953\n",
            "Support Vector Machine: Accuracy = 0.9421\n",
            "\n",
            "Evaluating ngram_range=(1, 2), max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.8938\n",
            "Logistic Regression: Accuracy = 0.9363\n",
            "Passive Aggressive Classifier: Accuracy = 0.9167\n",
            "Decision Tree Classifier: Accuracy = 0.9170\n",
            "Gradient Boosting Classifier: Accuracy = 0.9168\n",
            "Random Forest Classifier: Accuracy = 0.9445\n",
            "K-Nearest Neighbor: Accuracy = 0.8691\n",
            "Support Vector Machine: Accuracy = 0.9478\n",
            "\n",
            "Evaluating ngram_range=(1, 2), max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9287\n",
            "Logistic Regression: Accuracy = 0.9473\n",
            "Passive Aggressive Classifier: Accuracy = 0.9491\n",
            "Decision Tree Classifier: Accuracy = 0.9215\n",
            "Gradient Boosting Classifier: Accuracy = 0.9145\n",
            "Random Forest Classifier: Accuracy = 0.9489\n",
            "K-Nearest Neighbor: Accuracy = 0.8936\n",
            "Support Vector Machine: Accuracy = 0.9590\n",
            "\n",
            "Evaluating ngram_range=(1, 2), max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9374\n",
            "Logistic Regression: Accuracy = 0.9512\n",
            "Passive Aggressive Classifier: Accuracy = 0.9557\n",
            "Decision Tree Classifier: Accuracy = 0.9234\n",
            "Gradient Boosting Classifier: Accuracy = 0.9162\n",
            "Random Forest Classifier: Accuracy = 0.9528\n",
            "K-Nearest Neighbor: Accuracy = 0.8994\n",
            "Support Vector Machine: Accuracy = 0.9590\n",
            "\n",
            "Evaluating ngram_range=(1, 2), max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9304\n",
            "Logistic Regression: Accuracy = 0.9432\n",
            "Passive Aggressive Classifier: Accuracy = 0.9586\n",
            "Decision Tree Classifier: Accuracy = 0.9262\n",
            "Gradient Boosting Classifier: Accuracy = 0.9178\n",
            "Random Forest Classifier: Accuracy = 0.9515\n",
            "K-Nearest Neighbor: Accuracy = 0.8789\n",
            "Support Vector Machine: Accuracy = 0.9528\n",
            "\n",
            "Evaluating ngram_range=(1, 3), max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.8940\n",
            "Logistic Regression: Accuracy = 0.9365\n",
            "Passive Aggressive Classifier: Accuracy = 0.9114\n",
            "Decision Tree Classifier: Accuracy = 0.9163\n",
            "Gradient Boosting Classifier: Accuracy = 0.9174\n",
            "Random Forest Classifier: Accuracy = 0.9460\n",
            "K-Nearest Neighbor: Accuracy = 0.8682\n",
            "Support Vector Machine: Accuracy = 0.9483\n",
            "\n",
            "Evaluating ngram_range=(1, 3), max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9284\n",
            "Logistic Regression: Accuracy = 0.9483\n",
            "Passive Aggressive Classifier: Accuracy = 0.9466\n",
            "Decision Tree Classifier: Accuracy = 0.9196\n",
            "Gradient Boosting Classifier: Accuracy = 0.9170\n",
            "Random Forest Classifier: Accuracy = 0.9502\n",
            "K-Nearest Neighbor: Accuracy = 0.8914\n",
            "Support Vector Machine: Accuracy = 0.9598\n",
            "\n",
            "Evaluating ngram_range=(1, 3), max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9352\n",
            "Logistic Regression: Accuracy = 0.9516\n",
            "Passive Aggressive Classifier: Accuracy = 0.9569\n",
            "Decision Tree Classifier: Accuracy = 0.9215\n",
            "Gradient Boosting Classifier: Accuracy = 0.9176\n",
            "Random Forest Classifier: Accuracy = 0.9530\n",
            "K-Nearest Neighbor: Accuracy = 0.8984\n",
            "Support Vector Machine: Accuracy = 0.9599\n",
            "\n",
            "Evaluating ngram_range=(1, 3), max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9377\n",
            "Logistic Regression: Accuracy = 0.9469\n",
            "Passive Aggressive Classifier: Accuracy = 0.9598\n",
            "Decision Tree Classifier: Accuracy = 0.9215\n",
            "Gradient Boosting Classifier: Accuracy = 0.9167\n",
            "Random Forest Classifier: Accuracy = 0.9503\n",
            "K-Nearest Neighbor: Accuracy = 0.8942\n",
            "Support Vector Machine: Accuracy = 0.9557\n",
            "\n",
            "Evaluating ngram_range=(1, 4), max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.8947\n",
            "Logistic Regression: Accuracy = 0.9365\n",
            "Passive Aggressive Classifier: Accuracy = 0.9141\n",
            "Decision Tree Classifier: Accuracy = 0.9151\n",
            "Gradient Boosting Classifier: Accuracy = 0.9166\n",
            "Random Forest Classifier: Accuracy = 0.9445\n",
            "K-Nearest Neighbor: Accuracy = 0.8691\n",
            "Support Vector Machine: Accuracy = 0.9487\n",
            "\n",
            "Evaluating ngram_range=(1, 4), max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9275\n",
            "Logistic Regression: Accuracy = 0.9486\n",
            "Passive Aggressive Classifier: Accuracy = 0.9464\n",
            "Decision Tree Classifier: Accuracy = 0.9166\n",
            "Gradient Boosting Classifier: Accuracy = 0.9162\n",
            "Random Forest Classifier: Accuracy = 0.9507\n",
            "K-Nearest Neighbor: Accuracy = 0.8927\n",
            "Support Vector Machine: Accuracy = 0.9598\n",
            "\n",
            "Evaluating ngram_range=(1, 4), max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9344\n",
            "Logistic Regression: Accuracy = 0.9507\n",
            "Passive Aggressive Classifier: Accuracy = 0.9548\n",
            "Decision Tree Classifier: Accuracy = 0.9205\n",
            "Gradient Boosting Classifier: Accuracy = 0.9180\n",
            "Random Forest Classifier: Accuracy = 0.9499\n",
            "K-Nearest Neighbor: Accuracy = 0.9002\n",
            "Support Vector Machine: Accuracy = 0.9590\n",
            "\n",
            "Evaluating ngram_range=(1, 4), max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9381\n",
            "Logistic Regression: Accuracy = 0.9490\n",
            "Passive Aggressive Classifier: Accuracy = 0.9595\n",
            "Decision Tree Classifier: Accuracy = 0.9217\n",
            "Gradient Boosting Classifier: Accuracy = 0.9166\n",
            "Random Forest Classifier: Accuracy = 0.9502\n",
            "K-Nearest Neighbor: Accuracy = 0.9004\n",
            "Support Vector Machine: Accuracy = 0.9564\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize stop words and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "# Load datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Preprocess data\n",
        "fake_news_df['Label'] = 0\n",
        "true_news_df['Label'] = 1\n",
        "combined_df = pd.concat([fake_news_df, true_news_df], ignore_index=True)\n",
        "combined_df.dropna(subset=['Text', 'Region'], inplace=True)\n",
        "combined_df['Processed_Content'] = combined_df['Text'] + \" \" + combined_df['Region']\n",
        "combined_df['Processed_Content'] = combined_df['Processed_Content'].apply(preprocess_text)\n",
        "\n",
        "# Shuffle the data\n",
        "combined_df = shuffle(combined_df, random_state=42)\n",
        "\n",
        "# Prepare data\n",
        "processed_content = combined_df['Processed_Content']\n",
        "y = combined_df['Label']\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"Multinomial NB\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Passive Aggressive Classifier\": PassiveAggressiveClassifier(max_iter=1000),\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "    \"K-Nearest Neighbor\": KNeighborsClassifier(),\n",
        "    \"Support Vector Machine\": SVC()\n",
        "}\n",
        "\n",
        "# Function to perform cross-validation\n",
        "def perform_cross_validation(classifier, X, y, cv=5):\n",
        "    scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "# N-gram range settings and maximum features options\n",
        "ngram_ranges = [(1, 1), (1, 2), (1, 3), (1, 4)]\n",
        "max_features_options = [1000, 5000, 10000, 50000]\n",
        "\n",
        "# Evaluate classifiers with different ngram_range and max_features settings\n",
        "for ngram_range in ngram_ranges:\n",
        "    for max_features in max_features_options:\n",
        "        print(f\"\\nEvaluating ngram_range={ngram_range}, max_features={max_features}:\")\n",
        "        vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "        X = vectorizer.fit_transform(processed_content)\n",
        "\n",
        "        for name, classifier in classifiers.items():\n",
        "            accuracy = perform_cross_validation(classifier, X, y, cv=5)\n",
        "            print(f\"{name}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8Hvrcnqq2CB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1J54DH4fq3BG",
        "outputId": "aaf5b82f-6f8b-4306-d5c0-a9c04048a8c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating ngram_range=(1, 1), max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.8826\n",
            "Logistic Regression: Accuracy = 0.9267\n",
            "Passive Aggressive Classifier: Accuracy = 0.9022\n",
            "Decision Tree Classifier: Accuracy = 0.8867\n",
            "Gradient Boosting Classifier: Accuracy = 0.9020\n",
            "Random Forest Classifier: Accuracy = 0.9320\n",
            "K-Nearest Neighbor: Accuracy = 0.8587\n",
            "Support Vector Machine: Accuracy = 0.9282\n",
            "\n",
            "Evaluating ngram_range=(1, 1), max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9130\n",
            "Logistic Regression: Accuracy = 0.9365\n",
            "Passive Aggressive Classifier: Accuracy = 0.9249\n",
            "Decision Tree Classifier: Accuracy = 0.9034\n",
            "Gradient Boosting Classifier: Accuracy = 0.9034\n",
            "Random Forest Classifier: Accuracy = 0.9367\n",
            "K-Nearest Neighbor: Accuracy = 0.8028\n",
            "Support Vector Machine: Accuracy = 0.9319\n",
            "\n",
            "Evaluating ngram_range=(1, 1), max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9088\n",
            "Logistic Regression: Accuracy = 0.9366\n",
            "Passive Aggressive Classifier: Accuracy = 0.9255\n",
            "Decision Tree Classifier: Accuracy = 0.9076\n",
            "Gradient Boosting Classifier: Accuracy = 0.9025\n",
            "Random Forest Classifier: Accuracy = 0.9399\n",
            "K-Nearest Neighbor: Accuracy = 0.7459\n",
            "Support Vector Machine: Accuracy = 0.9323\n",
            "\n",
            "Evaluating ngram_range=(1, 1), max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9002\n",
            "Logistic Regression: Accuracy = 0.9366\n",
            "Passive Aggressive Classifier: Accuracy = 0.9251\n",
            "Decision Tree Classifier: Accuracy = 0.9067\n",
            "Gradient Boosting Classifier: Accuracy = 0.9023\n",
            "Random Forest Classifier: Accuracy = 0.9373\n",
            "K-Nearest Neighbor: Accuracy = 0.6966\n",
            "Support Vector Machine: Accuracy = 0.9344\n",
            "\n",
            "Evaluating ngram_range=(1, 2), max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.9005\n",
            "Logistic Regression: Accuracy = 0.9418\n",
            "Passive Aggressive Classifier: Accuracy = 0.9176\n",
            "Decision Tree Classifier: Accuracy = 0.9104\n",
            "Gradient Boosting Classifier: Accuracy = 0.9183\n",
            "Random Forest Classifier: Accuracy = 0.9437\n",
            "K-Nearest Neighbor: Accuracy = 0.8876\n",
            "Support Vector Machine: Accuracy = 0.9421\n",
            "\n",
            "Evaluating ngram_range=(1, 2), max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9329\n",
            "Logistic Regression: Accuracy = 0.9515\n",
            "Passive Aggressive Classifier: Accuracy = 0.9452\n",
            "Decision Tree Classifier: Accuracy = 0.9262\n",
            "Gradient Boosting Classifier: Accuracy = 0.9161\n",
            "Random Forest Classifier: Accuracy = 0.9512\n",
            "K-Nearest Neighbor: Accuracy = 0.8689\n",
            "Support Vector Machine: Accuracy = 0.9456\n",
            "\n",
            "Evaluating ngram_range=(1, 2), max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9382\n",
            "Logistic Regression: Accuracy = 0.9530\n",
            "Passive Aggressive Classifier: Accuracy = 0.9520\n",
            "Decision Tree Classifier: Accuracy = 0.9291\n",
            "Gradient Boosting Classifier: Accuracy = 0.9147\n",
            "Random Forest Classifier: Accuracy = 0.9522\n",
            "K-Nearest Neighbor: Accuracy = 0.8625\n",
            "Support Vector Machine: Accuracy = 0.9460\n",
            "\n",
            "Evaluating ngram_range=(1, 2), max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9336\n",
            "Logistic Regression: Accuracy = 0.9518\n",
            "Passive Aggressive Classifier: Accuracy = 0.9522\n",
            "Decision Tree Classifier: Accuracy = 0.9317\n",
            "Gradient Boosting Classifier: Accuracy = 0.9143\n",
            "Random Forest Classifier: Accuracy = 0.9502\n",
            "K-Nearest Neighbor: Accuracy = 0.5920\n",
            "Support Vector Machine: Accuracy = 0.9396\n",
            "\n",
            "Evaluating ngram_range=(1, 3), max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.9002\n",
            "Logistic Regression: Accuracy = 0.9416\n",
            "Passive Aggressive Classifier: Accuracy = 0.9072\n",
            "Decision Tree Classifier: Accuracy = 0.9133\n",
            "Gradient Boosting Classifier: Accuracy = 0.9190\n",
            "Random Forest Classifier: Accuracy = 0.9431\n",
            "K-Nearest Neighbor: Accuracy = 0.8884\n",
            "Support Vector Machine: Accuracy = 0.9410\n",
            "\n",
            "Evaluating ngram_range=(1, 3), max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9313\n",
            "Logistic Regression: Accuracy = 0.9532\n",
            "Passive Aggressive Classifier: Accuracy = 0.9445\n",
            "Decision Tree Classifier: Accuracy = 0.9262\n",
            "Gradient Boosting Classifier: Accuracy = 0.9178\n",
            "Random Forest Classifier: Accuracy = 0.9528\n",
            "K-Nearest Neighbor: Accuracy = 0.8712\n",
            "Support Vector Machine: Accuracy = 0.9460\n",
            "\n",
            "Evaluating ngram_range=(1, 3), max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9371\n",
            "Logistic Regression: Accuracy = 0.9539\n",
            "Passive Aggressive Classifier: Accuracy = 0.9495\n",
            "Decision Tree Classifier: Accuracy = 0.9287\n",
            "Gradient Boosting Classifier: Accuracy = 0.9153\n",
            "Random Forest Classifier: Accuracy = 0.9510\n",
            "K-Nearest Neighbor: Accuracy = 0.8625\n",
            "Support Vector Machine: Accuracy = 0.9458\n",
            "\n",
            "Evaluating ngram_range=(1, 3), max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9402\n",
            "Logistic Regression: Accuracy = 0.9530\n",
            "Passive Aggressive Classifier: Accuracy = 0.9519\n",
            "Decision Tree Classifier: Accuracy = 0.9308\n",
            "Gradient Boosting Classifier: Accuracy = 0.9163\n",
            "Random Forest Classifier: Accuracy = 0.9519\n",
            "K-Nearest Neighbor: Accuracy = 0.7024\n",
            "Support Vector Machine: Accuracy = 0.9407\n",
            "\n",
            "Evaluating ngram_range=(1, 4), max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.9002\n",
            "Logistic Regression: Accuracy = 0.9414\n",
            "Passive Aggressive Classifier: Accuracy = 0.9142\n",
            "Decision Tree Classifier: Accuracy = 0.9075\n",
            "Gradient Boosting Classifier: Accuracy = 0.9184\n",
            "Random Forest Classifier: Accuracy = 0.9448\n",
            "K-Nearest Neighbor: Accuracy = 0.8886\n",
            "Support Vector Machine: Accuracy = 0.9408\n",
            "\n",
            "Evaluating ngram_range=(1, 4), max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9304\n",
            "Logistic Regression: Accuracy = 0.9528\n",
            "Passive Aggressive Classifier: Accuracy = 0.9440\n",
            "Decision Tree Classifier: Accuracy = 0.9263\n",
            "Gradient Boosting Classifier: Accuracy = 0.9164\n",
            "Random Forest Classifier: Accuracy = 0.9527\n",
            "K-Nearest Neighbor: Accuracy = 0.8748\n",
            "Support Vector Machine: Accuracy = 0.9458\n",
            "\n",
            "Evaluating ngram_range=(1, 4), max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9367\n",
            "Logistic Regression: Accuracy = 0.9536\n",
            "Passive Aggressive Classifier: Accuracy = 0.9485\n",
            "Decision Tree Classifier: Accuracy = 0.9271\n",
            "Gradient Boosting Classifier: Accuracy = 0.9153\n",
            "Random Forest Classifier: Accuracy = 0.9507\n",
            "K-Nearest Neighbor: Accuracy = 0.8689\n",
            "Support Vector Machine: Accuracy = 0.9453\n",
            "\n",
            "Evaluating ngram_range=(1, 4), max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9425\n",
            "Logistic Regression: Accuracy = 0.9530\n",
            "Passive Aggressive Classifier: Accuracy = 0.9503\n",
            "Decision Tree Classifier: Accuracy = 0.9312\n",
            "Gradient Boosting Classifier: Accuracy = 0.9158\n",
            "Random Forest Classifier: Accuracy = 0.9548\n",
            "K-Nearest Neighbor: Accuracy = 0.7156\n",
            "Support Vector Machine: Accuracy = 0.9418\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize stop words and stemmer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "# Load datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Preprocess data\n",
        "fake_news_df['Label'] = 0\n",
        "true_news_df['Label'] = 1\n",
        "combined_df = pd.concat([fake_news_df, true_news_df], ignore_index=True)\n",
        "combined_df.dropna(subset=['Text', 'Region'], inplace=True)\n",
        "combined_df['Processed_Content'] = combined_df['Text'] + \" \" + combined_df['Region']\n",
        "combined_df['Processed_Content'] = combined_df['Processed_Content'].apply(preprocess_text)\n",
        "\n",
        "# Shuffle the data\n",
        "combined_df = shuffle(combined_df, random_state=42)\n",
        "\n",
        "# Prepare data\n",
        "processed_content = combined_df['Processed_Content']\n",
        "y = combined_df['Label']\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"Multinomial NB\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Passive Aggressive Classifier\": PassiveAggressiveClassifier(max_iter=1000),\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "    \"K-Nearest Neighbor\": KNeighborsClassifier(),\n",
        "    \"Support Vector Machine\": SVC()\n",
        "}\n",
        "\n",
        "# Function to perform cross-validation\n",
        "def perform_cross_validation(classifier, X, y, cv=5):\n",
        "    scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "# N-gram range settings and maximum features options\n",
        "ngram_ranges = [(1, 1), (1, 2), (1, 3), (1, 4)]\n",
        "max_features_options = [1000, 5000, 10000, 50000]\n",
        "\n",
        "# Evaluate classifiers with different ngram_range and max_features settings\n",
        "for ngram_range in ngram_ranges:\n",
        "    for max_features in max_features_options:\n",
        "        print(f\"\\nEvaluating ngram_range={ngram_range}, max_features={max_features}:\")\n",
        "        vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "        X = vectorizer.fit_transform(processed_content)\n",
        "\n",
        "        for name, classifier in classifiers.items():\n",
        "            accuracy = perform_cross_validation(classifier, X, y, cv=5)\n",
        "            print(f\"{name}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKUqyU2drNus"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l1_IM15UrN8Q",
        "outputId": "1ce3e7f1-cdc5-4b01-ab49-ffa7553f311c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating ngram_range=(1, 1), max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.8859\n",
            "Logistic Regression: Accuracy = 0.9270\n",
            "Passive Aggressive Classifier: Accuracy = 0.8998\n",
            "Decision Tree Classifier: Accuracy = 0.8881\n",
            "Gradient Boosting Classifier: Accuracy = 0.9009\n",
            "Random Forest Classifier: Accuracy = 0.9320\n",
            "K-Nearest Neighbor: Accuracy = 0.8432\n",
            "Support Vector Machine: Accuracy = 0.9300\n",
            "\n",
            "Evaluating ngram_range=(1, 1), max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9134\n",
            "Logistic Regression: Accuracy = 0.9367\n",
            "Passive Aggressive Classifier: Accuracy = 0.9253\n",
            "Decision Tree Classifier: Accuracy = 0.9080\n",
            "Gradient Boosting Classifier: Accuracy = 0.9008\n",
            "Random Forest Classifier: Accuracy = 0.9375\n",
            "K-Nearest Neighbor: Accuracy = 0.8089\n",
            "Support Vector Machine: Accuracy = 0.9327\n",
            "\n",
            "Evaluating ngram_range=(1, 1), max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9108\n",
            "Logistic Regression: Accuracy = 0.9357\n",
            "Passive Aggressive Classifier: Accuracy = 0.9294\n",
            "Decision Tree Classifier: Accuracy = 0.9064\n",
            "Gradient Boosting Classifier: Accuracy = 0.9017\n",
            "Random Forest Classifier: Accuracy = 0.9400\n",
            "K-Nearest Neighbor: Accuracy = 0.7434\n",
            "Support Vector Machine: Accuracy = 0.9319\n",
            "\n",
            "Evaluating ngram_range=(1, 1), max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9045\n",
            "Logistic Regression: Accuracy = 0.9357\n",
            "Passive Aggressive Classifier: Accuracy = 0.9292\n",
            "Decision Tree Classifier: Accuracy = 0.9105\n",
            "Gradient Boosting Classifier: Accuracy = 0.9020\n",
            "Random Forest Classifier: Accuracy = 0.9374\n",
            "K-Nearest Neighbor: Accuracy = 0.6985\n",
            "Support Vector Machine: Accuracy = 0.9334\n",
            "\n",
            "Evaluating ngram_range=(1, 2), max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.9034\n",
            "Logistic Regression: Accuracy = 0.9414\n",
            "Passive Aggressive Classifier: Accuracy = 0.9068\n",
            "Decision Tree Classifier: Accuracy = 0.9125\n",
            "Gradient Boosting Classifier: Accuracy = 0.9197\n",
            "Random Forest Classifier: Accuracy = 0.9436\n",
            "K-Nearest Neighbor: Accuracy = 0.8947\n",
            "Support Vector Machine: Accuracy = 0.9415\n",
            "\n",
            "Evaluating ngram_range=(1, 2), max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9342\n",
            "Logistic Regression: Accuracy = 0.9527\n",
            "Passive Aggressive Classifier: Accuracy = 0.9462\n",
            "Decision Tree Classifier: Accuracy = 0.9287\n",
            "Gradient Boosting Classifier: Accuracy = 0.9159\n",
            "Random Forest Classifier: Accuracy = 0.9512\n",
            "K-Nearest Neighbor: Accuracy = 0.8776\n",
            "Support Vector Machine: Accuracy = 0.9460\n",
            "\n",
            "Evaluating ngram_range=(1, 2), max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9394\n",
            "Logistic Regression: Accuracy = 0.9533\n",
            "Passive Aggressive Classifier: Accuracy = 0.9483\n",
            "Decision Tree Classifier: Accuracy = 0.9295\n",
            "Gradient Boosting Classifier: Accuracy = 0.9138\n",
            "Random Forest Classifier: Accuracy = 0.9531\n",
            "K-Nearest Neighbor: Accuracy = 0.8596\n",
            "Support Vector Machine: Accuracy = 0.9458\n",
            "\n",
            "Evaluating ngram_range=(1, 2), max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9352\n",
            "Logistic Regression: Accuracy = 0.9504\n",
            "Passive Aggressive Classifier: Accuracy = 0.9522\n",
            "Decision Tree Classifier: Accuracy = 0.9328\n",
            "Gradient Boosting Classifier: Accuracy = 0.9130\n",
            "Random Forest Classifier: Accuracy = 0.9477\n",
            "K-Nearest Neighbor: Accuracy = 0.5876\n",
            "Support Vector Machine: Accuracy = 0.9394\n",
            "\n",
            "Evaluating ngram_range=(1, 3), max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.9031\n",
            "Logistic Regression: Accuracy = 0.9412\n",
            "Passive Aggressive Classifier: Accuracy = 0.9153\n",
            "Decision Tree Classifier: Accuracy = 0.9110\n",
            "Gradient Boosting Classifier: Accuracy = 0.9180\n",
            "Random Forest Classifier: Accuracy = 0.9452\n",
            "K-Nearest Neighbor: Accuracy = 0.8958\n",
            "Support Vector Machine: Accuracy = 0.9421\n",
            "\n",
            "Evaluating ngram_range=(1, 3), max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9309\n",
            "Logistic Regression: Accuracy = 0.9520\n",
            "Passive Aggressive Classifier: Accuracy = 0.9448\n",
            "Decision Tree Classifier: Accuracy = 0.9267\n",
            "Gradient Boosting Classifier: Accuracy = 0.9166\n",
            "Random Forest Classifier: Accuracy = 0.9516\n",
            "K-Nearest Neighbor: Accuracy = 0.8785\n",
            "Support Vector Machine: Accuracy = 0.9456\n",
            "\n",
            "Evaluating ngram_range=(1, 3), max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9367\n",
            "Logistic Regression: Accuracy = 0.9539\n",
            "Passive Aggressive Classifier: Accuracy = 0.9493\n",
            "Decision Tree Classifier: Accuracy = 0.9298\n",
            "Gradient Boosting Classifier: Accuracy = 0.9150\n",
            "Random Forest Classifier: Accuracy = 0.9523\n",
            "K-Nearest Neighbor: Accuracy = 0.8690\n",
            "Support Vector Machine: Accuracy = 0.9464\n",
            "\n",
            "Evaluating ngram_range=(1, 3), max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9424\n",
            "Logistic Regression: Accuracy = 0.9522\n",
            "Passive Aggressive Classifier: Accuracy = 0.9519\n",
            "Decision Tree Classifier: Accuracy = 0.9312\n",
            "Gradient Boosting Classifier: Accuracy = 0.9147\n",
            "Random Forest Classifier: Accuracy = 0.9531\n",
            "K-Nearest Neighbor: Accuracy = 0.7190\n",
            "Support Vector Machine: Accuracy = 0.9399\n",
            "\n",
            "Evaluating ngram_range=(1, 4), max_features=1000:\n",
            "Multinomial NB: Accuracy = 0.9034\n",
            "Logistic Regression: Accuracy = 0.9415\n",
            "Passive Aggressive Classifier: Accuracy = 0.9130\n",
            "Decision Tree Classifier: Accuracy = 0.9113\n",
            "Gradient Boosting Classifier: Accuracy = 0.9196\n",
            "Random Forest Classifier: Accuracy = 0.9441\n",
            "K-Nearest Neighbor: Accuracy = 0.8909\n",
            "Support Vector Machine: Accuracy = 0.9419\n",
            "\n",
            "Evaluating ngram_range=(1, 4), max_features=5000:\n",
            "Multinomial NB: Accuracy = 0.9303\n",
            "Logistic Regression: Accuracy = 0.9515\n",
            "Passive Aggressive Classifier: Accuracy = 0.9433\n",
            "Decision Tree Classifier: Accuracy = 0.9251\n",
            "Gradient Boosting Classifier: Accuracy = 0.9176\n",
            "Random Forest Classifier: Accuracy = 0.9518\n",
            "K-Nearest Neighbor: Accuracy = 0.8807\n",
            "Support Vector Machine: Accuracy = 0.9454\n",
            "\n",
            "Evaluating ngram_range=(1, 4), max_features=10000:\n",
            "Multinomial NB: Accuracy = 0.9381\n",
            "Logistic Regression: Accuracy = 0.9543\n",
            "Passive Aggressive Classifier: Accuracy = 0.9497\n",
            "Decision Tree Classifier: Accuracy = 0.9307\n",
            "Gradient Boosting Classifier: Accuracy = 0.9147\n",
            "Random Forest Classifier: Accuracy = 0.9514\n",
            "K-Nearest Neighbor: Accuracy = 0.8784\n",
            "Support Vector Machine: Accuracy = 0.9461\n",
            "\n",
            "Evaluating ngram_range=(1, 4), max_features=50000:\n",
            "Multinomial NB: Accuracy = 0.9441\n",
            "Logistic Regression: Accuracy = 0.9522\n",
            "Passive Aggressive Classifier: Accuracy = 0.9494\n",
            "Decision Tree Classifier: Accuracy = 0.9336\n",
            "Gradient Boosting Classifier: Accuracy = 0.9151\n",
            "Random Forest Classifier: Accuracy = 0.9512\n",
            "K-Nearest Neighbor: Accuracy = 0.7163\n",
            "Support Vector Machine: Accuracy = 0.9410\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Initialize stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Load datasets\n",
        "fake_news_df = pd.read_csv('fakeNews.csv')\n",
        "true_news_df = pd.read_csv('trueNews.csv')\n",
        "\n",
        "# Preprocess data\n",
        "fake_news_df['Label'] = 0\n",
        "true_news_df['Label'] = 1\n",
        "combined_df = pd.concat([fake_news_df, true_news_df], ignore_index=True)\n",
        "combined_df.dropna(subset=['Text', 'Region'], inplace=True)\n",
        "combined_df['Processed_Content'] = combined_df['Text'] + \" \" + combined_df['Region']\n",
        "combined_df['Processed_Content'] = combined_df['Processed_Content'].apply(preprocess_text)\n",
        "\n",
        "# Shuffle the data\n",
        "combined_df = shuffle(combined_df, random_state=42)\n",
        "\n",
        "# Prepare data\n",
        "processed_content = combined_df['Processed_Content']\n",
        "y = combined_df['Label']\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"Multinomial NB\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Passive Aggressive Classifier\": PassiveAggressiveClassifier(max_iter=1000),\n",
        "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
        "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
        "    \"K-Nearest Neighbor\": KNeighborsClassifier(),\n",
        "    \"Support Vector Machine\": SVC()\n",
        "}\n",
        "\n",
        "# Function to perform cross-validation\n",
        "def perform_cross_validation(classifier, X, y, cv=5):\n",
        "    scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "# N-gram range settings and maximum features options\n",
        "ngram_ranges = [(1, 1), (1, 2), (1, 3), (1, 4)]\n",
        "max_features_options = [1000, 5000, 10000, 50000]\n",
        "\n",
        "# Evaluate classifiers with different ngram_range and max_features settings\n",
        "for ngram_range in ngram_ranges:\n",
        "    for max_features in max_features_options:\n",
        "        print(f\"\\nEvaluating ngram_range={ngram_range}, max_features={max_features}:\")\n",
        "        vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "        X = vectorizer.fit_transform(processed_content)\n",
        "\n",
        "        for name, classifier in classifiers.items():\n",
        "            accuracy = perform_cross_validation(classifier, X, y, cv=5)\n",
        "            print(f\"{name}: Accuracy = {accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMvR2awmcc24qavzRtE56GZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}